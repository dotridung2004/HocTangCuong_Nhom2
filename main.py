# ===============================
# file: main.py
# ===============================
import os
import numpy as np
from env.gridworld import GridWorld
from utils.map_io import load_map_json
from algorithms.value_iteration import value_iteration
from algorithms.policy_iteration import policy_iteration
from algorithms.q_learning import q_learning
from algorithms.sarsa import sarsa
from utils.visualize import plot_grid, draw_policy, plot_learning_curve


# ===============================
# 1ï¸âƒ£ Cháº¡y cÃ¡c thuáº­t toÃ¡n Dynamic Programming
# ===============================
def run_dp(env):
    print("â–¶ Running Value Iteration...")
    V_vi, policy_vi = value_iteration(env, gamma=0.9)
    draw_policy(env.grid, policy_vi,
                title="Policy - Value Iteration",
                savepath="results/policy_vi.png")

    print("â–¶ Running Policy Iteration...")
    V_pi, policy_pi = policy_iteration(env, gamma=0.9)
    draw_policy(env.grid, policy_pi,
                title="Policy - Policy Iteration",
                savepath="results/policy_pi.png")

    return policy_vi, policy_pi


# ===============================
# 2ï¸âƒ£ Q-learning
# ===============================
def run_q_learning(env):
    print("â–¶ Running Q-learning...")
    Q, rewards, steps = q_learning(
        env,
        num_episodes=1500,
        alpha=0.1,
        gamma=0.9,
        epsilon=1.0,
        epsilon_decay=0.995,
        seed=42
    )
    policy = {s: int(np.argmax(arr)) for s, arr in Q.items()}
    draw_policy(env.grid, policy,
                title="Policy - Q-learning",
                savepath="results/policy_qlearning.png")
    plot_learning_curve(rewards,
                        title="Q-learning Rewards",
                        savepath="results/learning_qlearning.png")
    return policy


# ===============================
# 3ï¸âƒ£ SARSA
# ===============================
def run_sarsa(env):
    print("â–¶ Running SARSA...")
    Q, rewards, steps = sarsa(
        env,
        num_episodes=1500,
        alpha=0.1,
        gamma=0.9,
        epsilon=1.0,
        epsilon_decay=0.995,
        seed=24
    )
    policy = {s: int(np.argmax(arr)) for s, arr in Q.items()}
    draw_policy(env.grid, policy,
                title="Policy - SARSA",
                savepath="results/policy_sarsa.png")
    plot_learning_curve(rewards,
                        title="SARSA Rewards",
                        savepath="results/learning_sarsa.png")
    return policy


# ===============================
# 4ï¸âƒ£ ÄÃ¡nh giÃ¡ chÃ­nh sÃ¡ch
# ===============================
def evaluate_policy(env, policy, episodes=50):
    successes, steps_list = 0, []
    for ep in range(episodes):
        state = env.reset()
        done, steps = False, 0
        while not done:
            a = policy.get(state, None)
            if a is None:
                break
            state, r, done, _ = env.step(a)
            steps += 1
            if steps > env.max_steps:
                break
        # náº¿u cháº¡m Ä‘Ã­ch
        if tuple(env.pos) in env.goals:
            successes += 1
            steps_list.append(steps)
    success_rate = successes / episodes * 100
    avg_steps = np.mean(steps_list) if steps_list else None
    return success_rate, avg_steps


# ===============================
# 5ï¸âƒ£ ChÆ°Æ¡ng trÃ¬nh chÃ­nh
# ===============================
def main():
    os.makedirs("results", exist_ok=True)

    map_path = "maps/dhtl_map.json"
    if not os.path.exists(map_path):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file báº£n Ä‘á»“: {map_path}")
        print("ğŸ‘‰ HÃ£y cháº¡y convert_map.py trÆ°á»›c Ä‘á»ƒ táº¡o file JSON tá»« áº£nh.")
        return

    # --- Load map ---
    print(f"ğŸ“‚ Loading map from: {map_path}")
    grid = load_map_json(map_path)

    # --- Khá»Ÿi táº¡o mÃ´i trÆ°á»ng ---
    env = GridWorld(
        grid,
        step_reward=-1,
        wall_reward=-5,
        goal_reward=100,
        max_steps=800,
        seed=123
    )

    # --- Váº½ báº£n Ä‘á»“ ---
    plot_grid(env.base_grid, title="DHTL Map (Start & Goal)", savepath="results/map.png")

    # --- Cháº¡y Value Iteration + Policy Iteration ---
    pi_vi, pi_pi = run_dp(env)

    # --- Cháº¡y Q-learning & SARSA ---
    pi_ql = run_q_learning(env)
    pi_sarsa = run_sarsa(env)

    # --- ÄÃ¡nh giÃ¡ ---
    print("\nğŸ“Š ÄÃ¡nh giÃ¡ cÃ¡c chÃ­nh sÃ¡ch:")
    for name, policy in [
        ("Value Iteration", pi_vi),
        ("Policy Iteration", pi_pi),
        ("Q-learning", pi_ql),
        ("SARSA", pi_sarsa)
    ]:
        sr, avg = evaluate_policy(env, policy, episodes=50)
        print(f"- {name:16}: Success {sr:.1f}% | Avg steps {avg}")


# ===============================
# 6ï¸âƒ£ Entry point
# ===============================
if __name__ == "__main__":
    main()
